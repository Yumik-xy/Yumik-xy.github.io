---
layout: post
title: 基于GMM-HMM的语音识别笔记（二）
date: 2021-05-26 18:32:00 +0800
categories: 语音识别
tag: 
- HMM
- GMM
---

* content
{:toc}
[数字语音处理 李琳山老师](http://speech.ee.ntu.edu.tw/DSP2019Spring/) 学习笔记（二）

### 马尔可夫模型

#### （一阶）马尔可夫链

在一般的数学定义上，马尔可夫模型状态转移是**随意的**，任意一个状态$s_i$可以以概率$a_{ij}$转移到状态$s_j$

但是语音信号中输入的声音一定是**顺序传递的**，如「ni’hao」一定是按「h-i-h-a-o」的顺序进行，而不可能按照「o-a-h-i-h」的逆序，因此声音信号的状态转移方向一定是单向的，即任意一个状态$s_i$以概率$a_{ij}$转移到状态$s_j\ (j\geq i$)

此外，数学定义的每一个状态即对应着一个固定的输出。例如：一个具有3个状态的马尔可夫模型

<img src="https://yumik-xy.oss-cn-qingdao.aliyuncs.com/img/20210526183935.png!small" alt="image-20210526183935889" style="zoom: 67%;" />

其中状态$s_1$对应的输出就是$A$，其中状态$s_2$对应的输出就是$B$，其中状态$s_3$对应的输出就是$C$，因此给出的任意一个已知序列：$\overline{O}=(CABABBA)$，对应的状态：$S=(s_3s_1s_2s_1s_2s_2s_1)$，其对应概率可以很轻易的计算出来

但是语音信号的状态并非对应着一个唯一的输出，其每一个状态含有N个输出，这N个输出又有各自的输出概率，这种含有隐含关系的马尔科夫链模型称为「隐马尔科夫模型」

#### 隐马尔科夫模型

下图是给出的隐马尔可夫模型例子，每一个状态都包含了三个输出，只是这三个输出在不同的状态下输出的概率不同

<img src="https://yumik-xy.oss-cn-qingdao.aliyuncs.com/img/20210526190635.png!small" alt="image-20210526190635942" style="zoom:67%;" />

反过来说同样的一个已知序列$\overline{O}=(ABC)$，其对应的状态我们并不知道，$s_1$、$s_2$、$s_3$都有可能，因此对应的状态组合一共有$3^3=27$种

所以**隐Hidden**：**状态State**是隐性的

[笔记（一）中「HMM参数」](https://yumik.top/语音识别/基于GMM-HMM的语音识别笔记-一#hmm参数) 说明了HMM输入$\lambda=HMM(A,B,\pi)$包含了三个值，则上图的HMM参数有：

$A=\begin{bmatrix}0.6  &0.3  &0.1 \\\\0.1  &0.7  &0.2 \\\\0.3  &0.2  &0.5\end{bmatrix}$

$b_1(A)=0.3,b_1(B)=0.2,b_1(C)=0.5$

$b_2(A)=0.7,b_2(B)=0.1,b_2(C)=0.3$

$b_3(A)=0.3,b_3(B)=0.6,b_3(C)=0.1$

$\pi=[0.4\quad 0.5\quad 0.1]$

举个例子状态$S=(s_2s_2s_3)$，则对应的概率=输出概率×状态概率即

$P(\overline{O}\|\overline{q},\lambda)P(\overline{q}\|\lambda)=(0.5\times0.7\times0.2)\times(0.7\times0.1\times0.1)=0.07\times0.007$

，0.5表示起始为$s_2$的概率，0.7表示状态$s_2$转到状态$s_2$的概率，以此类推……最终我们可以计算出这27可能的组合的概率，所有的概率和即为$P(\overline{O}\|\lambda)$

#### 求隐马尔科夫模型的方法

求解有三大主要问题，需要一一解决（难点）

##### 1. 评估问题

如何有效的计算$P(\overline{O}\|\lambda)$？

根据之前推导出的公式$P(\overline{O}\|\lambda)=\sum_{all\ \overline{q}}P(\overline{O}\|\overline{q},\lambda)P(\overline{q}\|\lambda)$，

得到：$P(\overline{O}\|\lambda)=\sum_{all\ \overline{q}}([b_{q1}(o_1)b_{q2}(o_2)\cdots b_{qT}(o_T)]\times[\pi_{q1}a_{q1q2}a_{q2q3}\cdots a_{(qT-1)(qT)}])$，

<img src="https://yumik-xy.oss-cn-qingdao.aliyuncs.com/img/20210526230501.png!small" alt="image-20210526230501796" style="zoom: 67%;" />

图中横轴表示随时间的输出$o_t$，纵轴表示状态$s_i$，如点$(o_3,1)$表示$o_3$这个向量是在$s_1$上取到的，而向量$(o_2,1)\to(o_3,1)$表示当前状态是$s_1$，下一个状态还是$s_1$，因此输出从$o_2$跳到$o_3$，因此这个二维的平面表示的路径就是前向算法的前进路径

我们定义前向向量为$\alpha_t(i)$

$\alpha_t(i)=P(o_1o_2\cdots o_n,q_t=i\|\lambda)=Prob(观测到o_1,o_2,cdots,o_{t-1}时的状态i在t[\lambda]时刻)$

<img src="https://yumik-xy.oss-cn-qingdao.aliyuncs.com/img/20210526231800.png!small" alt="image-20210526231800705" style="zoom:67%;" />

如何理解这一个公式呢，通过上图我们可以看到，如果我要得到$\alpha_{o_3}(3)$的值，也就是移动到点$(o_3,3)$，公式只规定了最后一点必须在目标点上，但是如何运动到这个点的并没有做任何要求，因此我们的路径可以在这个黄色框内**任意移动**，直到到达目标点

那么问题来了，为何说只能在黄色框内呢？

因为我们要求观测到输出为$o_3$时，状态i必须是在$t(\lambda)=3$时刻，因此我们能在$o_1,o_2$内任意行动，而一旦进入$o_3$必须使得$t(\lambda)=3$

> **不过以上我们并不是以「语音信号」的模型来讨论的**
>
> 在语音信号模型种，跳变的要求变得更加严格，因为我们的**语音信号一定是顺序的**，其输出只能从左往右走，状态变换只能从下往上走。例如「好hao」这个音，我设定$o_1=h$，$o_2=a$，$o_3=o$那么我可以把`h`这个音拖的很长很长，则：
>
> 可以得到这样的输出判决：h-h-h-h-h-a-o，和状态跳变：$s_1\to s_2\to\cdots\to s_n$，
>
> 不可能得到这样的输出判决：h-a-h-a-o，或是这样的状态跳变：$s_1\to s_3\to s_2\to\cdots$，因为这不是顺序的

回到刚才的问题，**我们想要求得什么？**

<img src="https://yumik-xy.oss-cn-qingdao.aliyuncs.com/img/20210527093146.png!small" alt="image-20210527093146106" style="zoom:67%;" />

如果我们可以求出所有的$\alpha_{t}(i)$的值，我们就一定可以推到得到$\alpha_{t+1}(j)$的值，即：

$\alpha_{t+1}(j)=[\sum_{i=1}^N\alpha_t(i)a_{ij}]b_j(o_{t+1})=1$，$1\leq j\leq N$，$1\leq t \leq T-1$

这个式子说明了任何一点$\alpha_t(i)$下一步要跳到$\alpha_{t+1}(j)$，其跳变概率为$a_{ij}$，因此所有$t$的点跳转到$\alpha_{t+1}(j)$就要乘以它们对应的跳变概率。并且跳变过去以后要输出一个唯一值，这个值也有其对应的概率$b_j(o_{t+1})$，因此就能求得$\alpha_{t+1}(j)$的概率

**初始：**$\alpha_1(i)=\pi_ib_i(o_1)$，$1\leq i\leq N$

**推演：**$\alpha_{t+1}(j)=[\sum_{i=1}^N\alpha_t(i)a_{ij}]b_j(o_{t+1})=1$，$1\leq j\leq N$

**最后：**$P(\overline{O}\|\lambda)=\sum_{i=1}^{N}\alpha_T(i)$

##### 2. 解码问题

如何选择最优状态序列？

给定$P(\overline{O}\|\lambda)$的$\overline{O}=o_1o_2\cdots o_T$和$\lambda$如何找到最佳句子$\overline{q}=q_1q_2\cdots q_T$

我们定义后向向量为$\beta_t(i)$

$\beta_t(i)=P(o_{t+1},o_{t+2},\cdots,o_T\|q_t=i,\lambda)=Prob[观测到o_{t+1},o_{t+2},\cdots,o_{T}时的状态i在t[\lambda]时刻]$

<img src="https://yumik-xy.oss-cn-qingdao.aliyuncs.com/img/20210527151811.png!small" alt="image-20210527151810784" style="zoom:67%;" />

我并不规定在黄色框内状态是如何转换的，我只要求进入$t+1$时刻之前，我的状态必须是**从$(t,i)$时刻进行**的跳变。但是$(t,i)$并不是我已知的，我只知道黄色框内的参数值和变化，我要反向推导出$(t,i)$的值，这就称为「后向算法」

**初始：**$\beta_T(i)=1$，$1\leq i\leq N$，意味着$\beta_{T-1}(i)=\sum_{j=1}^Na_{ij}b_j(o_T)$

**推演：**$\beta_t(i)=\sum_{j=1}^Na_{ij}b_j(o_{t+1})\beta_{t+1}$，$1\leq i\leq N$，$t=T-1,T-2,\cdots,2,1$

**最后：**$P(\overline{O}\|\lambda)=\sum_{i=1}^{N}[\alpha_t(i)\beta_t(i)$]



式子：$\beta_t(i)=\sum_{j=1}^Na_{ij}b_j(o_{t+1})\beta_{t+1}$通过一下图像进行理解

<img src="https://yumik-xy.oss-cn-qingdao.aliyuncs.com/img/20210601111546.png!small" alt="image-20210601111539421" style="zoom: 67%;" />

意味着，我从$(t,i)$开始，跳转到$j$的概率是$a_{ij}$，并且把蓝色点$(t+1,j)$显示出来的概率为$b_j(o_{t+1})$，我们再从这个点往后递推，所有跳转的概率和即为$\beta_t(i)$

当我们走到最后一行$\beta_T$时，没有$T+1$时刻了，所以$T$时刻是边界条件，需要设定初始值$\beta_T(i)=1$，这个设定是合理的。当$T-1$时刻需要从$T$时刻进行求解，但是跳转到$T$时刻后已经结束了，即$T$时刻不再需要后向跳转，故$\beta_{T-1}(i)=\sum_{j=1}^Na_{ij}b_j(o_T)$，其恰好是$\beta_T(i)=1$时候的递推表达式，故上面的假设成立



*因此以上两种「前向算法」和「后向算法」区别就在于欲求$(t,i)$的概率，但是只知道$T<t$时刻或$T>t$时刻的状态变化，即从前向后推导和从后向前推导的两种算法*



##### 3. 学习/训练问题

给定HMM的观测值$\overline{O}$，如何调整模型参数$\lambda=HMM(A,B,\pi)$使$$P(\overline{O}\|\lambda)$$最大？

